## 图像风格迁移(Neural Style)简史

面向读者：没有或有一定机器学习经验并对Prisma之类的app背后的原理感兴趣的读者。比较有经验的读者可以直接参照科技树阅读文章末罗列的引用论文。
阅读时间：10-20分钟
注：多图，请注意流量。



![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022922.jpg)

*图像风格迁移科技树*

## 序：什么是图像风格迁移？

先上一组图吧。以下每一张图都是一种不同的艺术风格。作为非艺术专业的人，我就不扯艺术风格是什么了，每个人都有每个人的见解，有些东西大概艺术界也没明确的定义。如何要把一个图像的风格变成另一种风格更是难以定义的问题。对于程序员，特别是对于机器学习方面的程序员来说，这种模糊的定义简直就是噩梦。到底怎么把一个说都说不清的东西变成一个可执行的程序，是困扰了很多图像风格迁移方面的研究者的问题。



![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022942.jpg)



在神经网络之前，图像风格迁移的程序有一个共同的思路：分析某一种风格的图像，给那一种风格建立一个数学或者统计模型，再改变要做迁移的图像让它能更好的符合建立的模型。这样做出来效果还是不错的，比如下面的三张图中所示，但一个很大的缺点：***一个程序基本只能做某一种风格或者某一个场景\***。因此基于传统风格迁移研究的实际应用非常有限。

![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-22941.jpg)



[景色照片时间迁移](https://link.zhihu.com/?target=http%3A//dl.acm.org/citation.cfm%3Fid%3D2508419)

改变了这种现状的是两篇Gatys的论文，在这之前让程序模仿任意一张图片画画是没法想象的。

![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022946.jpg)



*[第一个基于神经网络的图像风格迁移算法](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1508.06576)，生成时间：5-20分钟*

这篇文章中你不会看到数学公式，如果想要更加详细了解其中的数学的话可以阅读原论文。我想试着从头开始讲起，从[Gatys et al., 2015a](https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5633-texture-synthesis-using-convolutional-neural-networks)和[Gatys et al., 2015b](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1508.06576)中用到的一些技术的历史开始讲起，用最简单的方法说清楚基于神经网络的图像风格迁移的思路是什么，以及Gatys为什么能够想到使用神经网络来实现图像风格迁移。

如果大家对这个感兴趣的话，我将来可以继续写一些关于Neural Style最新的一些研究的进展，或者其他相关的一些图像生成类的研究，对抗网络之类的。写的有错误的不到位的地方请随意指正。

## Neural Style元年前20年-前3年

要理解对于计算机来说图片的风格是什么，只能追根溯源到2000年以及之前的图片纹理生成的研究上。明明是图像风格迁移的文章，为啥要说到图片纹理？在这儿我先卖个关子吧。

据我所知，在2015年前所有的关于图像纹理的论文都是手动建模的（比如 [A Parametric Texture Model Based on Joint Statistics of Complex Wavelet Coefficients](https://link.zhihu.com/?target=http%3A//www.springerlink.com/index/r244h74572250895.pdf)），其中用到的最重要的一个思想是：纹理可以用图像局部特征的统计模型来描述。没有这个前提一切模型无从谈起。什么是统计特征呢，简单的举个栗子



![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022940.jpg)

这个图片可以被称作栗子的纹理，这纹理有个特征，就是所有的栗子都有个开口，用简单的数学模型表示开口的话，就是两条大概某个弧度的弧线相交嘛，统计学上来说就是这种纹理有两条这个弧度的弧线相交的概率比较大，这种可以被称为统计特征。有了这个前提或者思想之后，研究者成功的用复杂的数学模型和公式来归纳和生成了一些纹理，但毕竟手工建模耗时耗力，（通俗的说，想象一下手工算栗子开口的数学模型，算出来的模型大概除了能套用在开心果上就没啥用了。。。）当时计算机计算能力还没现在的手机强，这方面的研究进展缓慢，十几年就这么过去了。

![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022925.jpg)

[早期纹理生成结果](https://link.zhihu.com/?target=http%3A//www.springerlink.com/index/r244h74572250895.pdf)

与此同时，隔壁的图像风格迁移也好不到哪里去，甚至比纹理生成还惨。因为纹理生成至少不管生成什么样子的纹理都叫纹理生成，然而图像风格迁移这个领域当时连个合适的名字都没有，因为每个风格的算法都是各管各的，互相之间并没有太多的共同之处。比如[油画风格迁移](https://link.zhihu.com/?target=http%3A//dl.acm.org/citation.cfm%3Fid%3D2811248)，里面用到了7种不同的步骤来描述和迁移油画的特征。又比如[头像风格迁移](https://link.zhihu.com/?target=https%3A//dspace.mit.edu/handle/1721.1/100018)里用到了三个步骤来把一种头像摄影风格迁移到另一种上。以上十个步骤里没一个重样的，可以看出图像风格处理的研究在2015年之前基本都是各自为战，捣鼓出来的算法也没引起什么注意。相比之下Photoshop虽然要手动修图，但比大部分算法好用多了。



![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022939.jpg)



[头像风格迁移](https://link.zhihu.com/?target=https%3A//dspace.mit.edu/handle/1721.1/100018)

![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022945.jpg)



[油画风格迁移](https://link.zhihu.com/?target=http%3A//dl.acm.org/citation.cfm%3Fid%3D2811248)

同一个时期，计算机领域进展最大的研究之一可以说是计算机图形学了。（这段有相关知识的可以跳过，不影响之后的阅读。）简单的来说计算机图形学就是现在几乎所有游戏的基础，不论是男友1(战地1)里穿越回一战的战斗场景，还是FGO之类的手游，背后都少不了一代又一代的图形学研究者的工作。在他们整日整夜忙着研究如何能让程序里的妹纸变成有血有肉的样子的时候，点科技树点出了一个重要的分支：显卡（GPU）。游戏机从刚诞生开始就伴随着显卡。显卡最大的功能当然是处理和显示图像。不同于CPU的是，CPU早期是单线程的，也就是一次只能处理一个任务，GPU可以一次同时处理很多任务，虽然单个任务的处理能力和速度比CPU差很多。比如一个128x128的超级马里奥游戏， 用CPU处理的话，每一帧都需要运行128x128=16384歩，而GPU因为可以同时计算所有像素点，时间上只需要1步，速度比CPU快很多。为了让游戏越来越逼近现实，显卡在过去20年内也变得越来越好。巧合的是，显卡计算能力的爆炸性增长直接导致了被放置play十几年的神经网络的复活和深度学习的崛起，因为神经网络和游戏图形计算的相似处是两者都需要对大量数据进行重复单一的计算。可以说如果没有游戏界就没有深度学习，也就没有Neural Style。所以想学机器学习先得去steam买买买支持显卡研究（误）。

![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-22938.jpg)

*ImageNet物体识别比赛中使用GPU的队伍数量逐年上升，错误率逐年下降*

提到神经网络我想稍微讲一下神经网络（特别是卷积神经网络）和传统做法的区别，已经有了解的可以跳过本段。卷积神经网络分为很多层，每一层都是由很多单个的人工神经元组成的。可以把每个神经元看作一个识别器，用刚刚的栗子来说的话，每一个或者几个神经元的组合都可以被用来识别某个特征，比如栗子的开口。在训练前它们都是随机的，所以啥都不能做，训练的过程中它们会自动的被变成一个个不同的识别器并且相互组合起来，大量的识别器组合起来之后就可以识别物体了。整个过程除了一开始的神经网络的设计和参数的调整之外其他全是自动的。这里我们就不介绍神经网络(Neural Network)和卷积神经网络(Convolutional Neural Network)具体怎么工作的了，如果对于神经网络具体怎么工作不了解的话，相信网上已经有很多很多相关的介绍和教程，有兴趣的可以去了解一下，不了解也不影响本文的阅读。

![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022935.jpg)

*卷积神经网络图例*

## Neural Style元年前3年-前1年

2012-2014年的时候深度学习刚开始火，火的一个主要原因是因为人们发现深度学习可以用来训练物体识别的模型。之前的物体识别模型有些是用几何形状和物体的不同部分比较来识别，有些按颜色，有些按3d建模，还有一些按照局部特征。传统物体识别算法中值得一提的是按照比较局部特征来识别物体，其原理如下：

比如我们的目标是在图片之中找到这个人：

![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022951.jpg)

*目标物体*
对于程序而言这个人就是一堆像素嘛，让它直接找的话它只能一个个像素的去比较然后返回最接近的了（近邻算法）。但是现实中物体的形状颜色会发生变化，如果手头又只有这一张照片，直接去找的速度和正确率实在太低。
有研究者想到，可以把这个人的照片拆成许多小块，然后一块一块的比较(方法叫Bag of Features)。最后哪一块区域相似的块数最多就把那片区域标出来。这种做法的好处在于即使识别一个小块出了问题，还有其他的小块能作为识别的依据，发生错误的风险比之前大大降低了。



![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022936.jpg)

*Bag of Features*

这种做法最大的缺点就是它还是把一个小块看成一坨像素然后按照像素的数值去比较，之前提到的改变光照改变形状导致物体无法被识别的问题根本上并没有得到解决。

用卷积神经网络做的物体识别器其实原理和bag of features差不了太多，***只是把有用的特征(feature)都装到了神经网络里了\***。刚提到了神经网络经过训练会自动提取最有用的特征，所以特征也不再只是单纯的把原来的物体一小块一小块的切开产生的，而是由神经网络选择最优的方式提取。



![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022926.jpg)

*卷积神经网络提取的特征示意图，每一格代表一个神经元最会被哪种图片激活。*

卷积神经网络当时最出名的一个物体识别网络之一叫做VGG19，结构如下：

![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022929.jpg)

*VGG19网络结构*
每一层神经网络都会利用上一层的输出来进一步提取更加复杂的特征，直到复杂到能被用来识别物体为止，***所以每一层都可以被看做很多个局部特征的提取器***。VGG19在物体识别方面的精度甩了之前的算法一大截，之后的物体识别系统也基本都改用深度学习了。

因为VGG19的优秀表现，引起了很多兴趣和讨论，但是VGG19具体内部在做什么其实很难理解，因为每一个神经元内部参数只是一堆数字而已。每个神经元有几百个输入和几百个输出，一个一个去梳理清楚神经元和神经元之间的关系太难。于是有人想出来一种办法：虽然我们不知道神经元是怎么工作的，但是如果我们知道了它的激活条件，会不会能对理解神经网络更有帮助呢？于是他们编了一个程序，（用的方法叫back propagation，和训练神经网络的方法一样，只是倒过来生成图片。）把每个神经元所对应的能激活它的图片找了出来，之前的那幅特征提取示意图就是这么生成的。有人在这之上又进一步，觉得，诶既然我们能找到一个神经元的激活条件，那能不能把所有关于“狗’的神经元找出来，让他们全部被激活，然后看看对于神经网络来说”狗“长什么样子的？
长得其实是这样的：

![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022928.jpg)

*神经网络想象中的狗*

这是神经网络想象中最完美的狗的样子，非常迷幻，感觉都可以自成一派搞个艺术风格出来了。而能把任何图片稍作修改让神经网络产生那就是狗的幻觉的程序被称作deep dream。

![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022943.jpg)

*Deep Dream*

## Neural Style元年

有了这么多铺垫，一切的要素已经凑齐，前置科技树也都已经被点亮了，终于可以进入正题了。基于神经网络的图像风格迁移在2015年由Gatys et al. 在两篇论文中提出：[Gatys et al., 2015a](https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5633-texture-synthesis-using-convolutional-neural-networks)和[Gatys et al., 2015b](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1508.06576)。我们先说第一篇。第一篇比起之前的纹理生成算法，创新点只有一个：它给了一种用深度学习来给纹理建模的方法。之前说到纹理生成的一个重要的假设是纹理能够通过局部统计模型来描述，而手动建模方法太麻烦。于是Gatys看了一眼隔壁的物体识别论文，发现VGG19说白了不就是一堆局部特征识别器嘛。他把事先训练好的网络拿过来一看，发现这些识别器还挺好用的。于是Gatys套了个Gramian matrix上去算了一下那些不同局部特征的相关性，把它变成了一个统计模型，于是就有了一个不用手工建模就能生成纹理的方法。

![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022944.jpg)

*基于神经网络的纹理生成算法*

从纹理到图片风格其实只差两步。第一步也是比较神奇的，是Gatys发现***纹理能够描述一个图像的风格\***。严格来说文理只是图片风格的一部分，但是不仔细研究纹理和风格之间的区别的话，乍一看给人感觉还真差不多。第二步是***如何只提取图片内容而不包括图片风格\***。这两点就是他的第二篇论文做的事情：Gatys又偷了个懒，把物体识别模型再拿出来用了一遍，这次不拿Gramian算统计模型了，直接把局部特征看做近似的图片内容，这样就得到了一个把图片内容和图片风格（说白了就是纹理）分开的系统，剩下的就是把一个图片的内容和另一个图片的风格合起来。合起来的方法用的正是之前提到的让神经网络“梦到”狗的方法，也就是研究员们玩出来的Deep Dream，找到能让合适的特征提取神经元被激活的图片即可。

![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022947.jpg)

*基于神经网络的图像风格迁移*

至此，我们就把关于基于神经网络的图像风格迁移(Neural Style)的重点解释清楚了。背后的每一步都是前人研究的结果，不用因为名字里带深度啊神经网络啊而感觉加了什么特技，特别的高级。Gatys所做的改进是把两个不同领域的研究成果有机的结合了起来，做出了令人惊艳的结果。其实最让我惊讶的是纹理竟然能够和人们心目中认识到的图片的风格在很大程度上相吻合。（和真正的艺术风格有很大区别，但是看上去挺好看的。。。）从那之后对neural style的改进也层出不穷，在这里就先放一些图，技术细节暂且不表。



![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022933.jpg)

*[改进后的图像风格迁移算法](https://link.zhihu.com/?target=http%3A//www.cv-foundation.org/openaccess/content_cvpr_2016/html/Li_Combining_Markov_Random_CVPR_2016_paper.html)，左：输入图像，中：改进前，右：改进后。生成时间：5-20分钟*



![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022937.jpg)

*[多个预设风格的融合](https://link.zhihu.com/?target=https%3A//research.google.com/pubs/pub45832.html)，生成时间：少于1秒，训练时间：每个风格1-10小时*



![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022949.jpg)

*[最新的实时任意风格迁移算法之一](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1612.04337)，生成时间：少于10秒（[少于一秒的算法](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1703.06868)也有，但个人认为看上去没这个好看），训练时间：10小时*



![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-022931.jpg)

*[图片类比](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1705.01088)，生成时间：5-20分钟*

最后安利一篇与本文无关的文章吧，[Research Debt](https://link.zhihu.com/?target=http%3A//distill.pub/2017/research-debt/) (原文为英语，相关知乎问题在[这里](https://www.zhihu.com/question/57639134)）是我写本文的动机。希望各位喜欢本文，也希望有余力的人能多写一些科普文。文笔不好献丑了。

## 引用：

注：排序基本按照时间顺序，带星号越多越重要，这里只引用了文章中提到过的论文，若有需要以后再加。

前置科技：

[A Parametric Texture Model Based on Joint Statistics of Complex Wavelet Coefficientswww.springerlink.com](https://link.zhihu.com/?target=http%3A//www.springerlink.com/index/r244h74572250895.pdf)[Data-driven hallucination of different times of day from a single outdoor photodl.acm.org](https://link.zhihu.com/?target=http%3A//dl.acm.org/citation.cfm%3Fid%3D2508419)[Image stylization by oil paint ﬁltering using color palettesdl.acm.org](https://link.zhihu.com/?target=http%3A//dl.acm.org/citation.cfm%3Fid%3D2811248)[**Texture synthesis using convolutional neural networkspapers.nips.cc](https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5633-texture-synthesis-using-convolutional-neural-networks)[*Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesiswww.cv-foundation.org](https://link.zhihu.com/?target=http%3A//www.cv-foundation.org/openaccess/content_cvpr_2016/html/Li_Combining_Markov_Random_CVPR_2016_paper.html)[A Learned Representation For Artistic Styleresearch.google.com](https://link.zhihu.com/?target=https%3A//research.google.com/pubs/pub45832.html)[*Arbitrary Style Transfer in Real-time with Adaptive Instance Normalizationarxiv.org](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1703.06868)

---

> *风格迁移是近来人工智能领域内的一个热门研究主题，此前各路媒体也报道了很多相关的研究。近日，来自浙江大学和亚利桑那州立大学的几位研究者在 arXiv 上发布了一篇「神经风格迁移（Neural Style Transfer）」的概述论文，对当前神经网络风格迁移技术的研究、应用和难题进行了全面的总结：[[1705.04058\] Neural Style Transfer: A Review](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1705.04058)。*

![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-075605.png)

**摘要**



Gatys 等人的近期研究证实了卷积神经网络（CNN）的力量：通过分离和重新组合图片内容与风格，CNN 可以创作出具有艺术魅力的作品。使用 CNN 将一张图片的语义内容与不同风格融合起来的过程被称为神经风格迁移（Neural Style Transfer）。从那以后，在学术研究和产业应用中，神经风格迁移已成为一个很受欢迎的主题，不仅日益受到计算机视觉研究者的关注，研究人员还提出了几种方法来提升或扩展 Gatys et al. 提出的神经算法。不过，有关这方面的全面综述、总结性文献还付之阙如。这篇论文回顾了神经网络风格迁移研究近期取得的进展，并讨论了这一技术的不同应用以及尚未解决的问题，这也是未来研究的方向。





*![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-075607.png)*

*图 1：使用了 Gatys 等人的风格迁移算法的例子，将中国画风格（b）转移到了一张长城相片（a）上。提供风格的那幅作品是黄公望的《富春山居图》。*





**1 引言**



本文的余下部分的逻辑结构如下。第 2 节对现有的神经风格迁移方法进行分类，并详细解释了这些方法。第 3、4 节介绍了这些方法的一些改进以及扩展。文章第 5 节给出了风格化输出效果的评估方法。第 6 节讨论了这些神经风格迁移方法的商业化应用。最后，第 7 节总结了当前面临的挑战以及可能的解决方案。第 8 部分总结全文并抛出了几个有前途的研究方向。



文章所涉论文以及相应的代码、预训练模型请移步至：[Build software better, together](https://link.zhihu.com/?target=https%3A//github.com/ycjing/Neural-Style-Transfer-Papers) ycjing/Neural-Style-Transfer-Papers



**2 神经风格迁移方法的二分法**



在这一部分，我们提出了一种分类方法。当前的神经风格迁移方法符合其中之一：基于图片迭代的描述性神经方法（Descriptive Neural Methods Based On Image Iteration）和基于模型迭代的生成式神经方法（Generative Neural Methods Based On Model Iteration）。第一类方法通过直接迭代更新图片像素来实现图像风格迁移，第二种方法首先会迭代优化生成模型，接着通过一个单独前向通过来生成风格化图像。



**2.1 基于图像迭代的描述性神经方法（Descriptive Neural Methods Based On Image Iteration）**



第一个用来转移图片风格的神经方法就是描述性神经方法。这一方法会从随机噪音开始，通过反向传播迭代更新（尚未知晓的）风格化图像。图像迭代的目标是最小化总损失，这样风格化后的图像就能同时将内容图像的内容与风格图像的风格匹配起来。



神经风格迁移的关键之一就是风格表征（representation of style），即预定义的风格损失函数。风格损失函数会被优化以匹配风格图像特征统计。根据采用的风格损失函数不同，我们可以进一步将这一方法分为基于最大均值差（MMD）的方法和基于马尔科夫随机场（MRF）的方法。简便起见，我们称之为基于 MMD 和基于 MRF 的方法。



**2.1.1 基于 MMD 的描述性神经方法**



MMD 是一个很受欢迎的评估两个分布之差的标准，以希尔伯特空间特征均值为基础［20］。最近，Li 等人的研究表明风格迁移能够被视为一个从内容图像到风格图像的分布对齐过程 [30]。因此，MMD 能被用于测量风格差异。基于 MMD 的描述性神经方法是指使用带有不同核函数的 MMD 作为优化的风格损失的神经方法。



**2.1.2 基于 MRF 的描述性神经方法**



MRF 是一种用于图像合成的经典框架。假设局部图像块包含了一张图像中的最相关统计依存性。描述性神经方法的第二类是以 MRF 为基础的，还考虑到了局部水平的神经风格迁移，比如局部图像块的风格匹配。



**2.2 基于模型迭代的生成式神经方法**



尽管描述性神经方法能够生成出色的风格化图像，但仍有局限性。其中之一就是效率问题。第二类，亦即基于模型迭代的生成式神经方法（在某些论文中，也被成为「快速」神经风格迁移）解决了速度和计算成本问题，不过牺牲了模型的灵活性。关键思想就是在每个针对特定风格图像的大型图像数据集上提前训练一个前馈网络。使用梯度下降，通过迭代式地更新模型来优化网络模型。



**3 当前方法上的轻微修改**



有几项研究提出了基于目前最好的神经网络风格迁移算法的改进版。这些改进版保留了现有算法的架构和处理过程，但稍稍改变了损失函数来获得更好的性能。





*![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-075605.png)*

*图 2：控制神经风格迁移中的画笔大小可以生成不同风格的结果。风格样式来自梵高的《The Starry Night》*





**3.1 描述性神经方法的衍生**



**3.2 生成式神经方法的衍生**



**4 特定类型图像的扩展**



上述神经风格迁移方法都是对静态图像进行处理的。它们可能不适用于其他类型的图像（如涂鸦、头像和视频）。目前已有很多研究正试图将最先进的神经风格迁移算法应用到这些特殊类型的图像，或指定目标的图像风格迁移中。



在涂鸦中的神经风格迁移。Champandard 完成了一些有趣的研究 [7]（如 2.1.2 节所示）。除了将语义映射引入到神经风格迁移算法中以外，人们也可以通过这种方法在图像中输入高级注释来把简单的草图转换为精细的图画。



头像的神经风格转移。尽管 Gatys 等人的算法已可以进行通用图像的风格转移，但它还不适用于头像的风格转移。由于空间约束不强，直接应用 Gatys 等人的方法可能会使人物头部变形。它对于这种类型的风格转移是不可接受的。Selim 等人 [41] 经过研究解决了这一问题，他们拓展了 Gatys 等人的算法。他们使用了增益图（gain maps）的概念来约束空间，从而在风格迁移的同时可以保留人物的面部轮廓。



指定对象的神经风格迁移。Castillo 等人 [5] 提出了指定风格迁移目标的算法。该算法是在图像中仅对单个用户指定的对象进行风格化的过程。这个想法是使用最先进的语义分割算法从风格化图像中分割目标对象，然后提取风格迁移后的对象与非风格化背景合并。



对视频的神经风格迁移。Ruder 等人 [40] 拓展了 Gatys 等人的研究，用神经风格前以算法对视频图像序列进行了处理，在本文中神经视频风格迁移中有提到。给定目标风格图片，Ruder 等人的算法引入时间损失函数来让整个视频都可以得到风格迁移。该算法背后的关键思想是使用时间约束来保持各帧之间的平滑过渡，即惩罚沿着点轨迹的偏差。Ruder 等人的算法已被证明可以能够在大多数情况下消除人工痕迹，并产生平滑的风格化视频。另一个此方向上的研究由 Anderson 等人 [3] 提出，它能够利用光流对风格迁移进行初始化，为一段影片进行渲染。



**5 评估方法**



对于神经风格迁移问题，并不存在什么 ground truth。神经风格迁移是一种艺术创作。不同的人可能会对同一个迁移后的结果有不同乃至截然相反的看法。因此，对神经风格迁移算法所得到的视觉结果的评估仍然还是一个悬而未决的重要问题。



从我们的角度来看，在神经风格迁移领域有两种类型的评估方法可以使用，即：定性评估（qualitative evaluation）和定量评估（quantitative evaluation）。定性评估要求参与者评估排序不同算法的结果，这依赖于参与者的观察（被称为「风格感知研究（Stylization Perceptual Studies）」）。这种评估结果可能会因参与者属性（比如年龄、职业）的不同而不同。尽管在定性评估方法上存在一定程度的不确定性，但该方法至少能够提供一些有关人们的神经艺术风格偏好的信息。而定量评估则侧重于在算法上的精确评估指标（比如时间复杂度）。



在目前的神经风格迁移领域，生成式的神经方法已经成为了一个热门的主题，其中速度是工业应用所考虑的主要问题之一。但就我们所知，之前还没有研究在同样的实验设置下运行过所有当前最佳的生成式神经方法，并在定性和定量上对它们进行比较。因此，在这一章中，我们的目标是比较 5 种当前最佳的生成式神经方法，并且使用了 Gatys et al. 的描述式神经方法作为参考。



实验设置。总体而来，实验使用了 10 张风格图像和 20 张内容图像。所有风格转换结果都是使用作者提供的代码 [43, 23, 27, 19, 8] 得到的，但 [14] 是例外。对于 [14]，我们使用了一种有所修改的（见第 3 章）流行的开源代码 [22]。我们的实验中使用的所有这些代码的参数都是原作者对应论文提供的默认参数，但 [12, 9] 是例外。我们为 [12,9] 使用了作者提供的预训练的模型。对于我们实验中所有的生成式神经方法，所有测试内容图像在训练过程中都没有被观察过。



**5.1 定性评估**





*![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-075613.png)*

*图 3：定性评估的一些结果示例*







*![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-075612.png)*

*表 1：对于图 3 中图像上的六种算法的平均风格排名分数（∈ [1, 6]）*





**5.2 定量评估**





*![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-075614.png)*

*表 2：在 256×256、512×512、1024×1024 三种像素大小上的神经风格迁移算法的速度比较（硬件：NVIDIA Quadro M6000）*





**6 应用**



因为神经风格迁移的结果惊人，所以也带来了很多成功的业界应用并且也开始实现了商业回报。同时，也有一些应用论文调查了神经风格迁移技术在不同应用领域的应用方式 [4,25]。本节对这些应用进行了总结，并提出了一些潜在的用途。



**6.1 社交**



**6.2 辅助用户创作的工具**



**6.3 用于娱乐应用的生产工具**



**7 难题与可能的解决方案**



神经风格迁移领域的进展惊人，在产业中也已有所应用。尽管目前的算法取得了惊人的结果，该领域仍有一些挑战与仍待解决的问题。在此章节中，我们将总结神经风格迁移领域内的问题，并讨论对应的解决方案。



**7.1 难题**



- 参数调整的问题
- 笔触方向控制问题
- 神经风格迁移中「快」与「更快」的问题





*![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-075615.png)*

*图 4：带有画笔大小控制的描述性神经方法的高分辨率结果，以及不带画笔大小控制的生成式神经方法的高分辨率结果。*





**7.2 可能的解决方案**



参数调整问题的解决方案。在未来自动参数调整问题的研究中，我们分别讨论了对描述性神经方法和生成式方法的解决方案。对描述性神经方法，可能的一个解决方案是跟随 Risser 等人的方法，进一步的结合一些无梯度信息，比如损失的量级以及损失内的统计。另一个方向是从分类问题中使用的自动参数优化策略获取灵感（例如，Domhan 等人的研究 [11]，Luo 的研究 [32]）。



对生成式神经方法，一种思路是研究不需要为不同风格训练单独模型的新方法（就像 [9]），同时还要保证结果的高质量。（也就是，突破速度、灵活性和质量之间的权衡）。然后参数调整的过程就不会特别耗费时间，而且把参数调整交给用户也可接受。此外，目前自动参数优化策略中的一些方法对生成式神经方法中的自动参数调整也会有帮助。



对笔触方向控制问题的可能解决方案。目前的神经风格迁移算法不考虑对笔触方向的控制。相反，在非真实感绘制技术（NPR：Non-photorealistic Rendering）领域，对笔触方向控制有很好的研究 [39]。我们相信 NPR 领域中的一些思路可被借鉴来解决神经风格迁移中的方向问题。例如，Zhang 等人要求用户明确笔触方向的位置与方式 [49]，因为不同的用户有不同的偏好。同样的思路可借鉴到神经风格迁移算法中，要求用户提前选择全局笔触方向。此外，结合神经风格迁移算法和 NPR 领域中指导笔触方向的策略（例如，[50] 中的向量场方法）是该问题的另一个潜在的解决方案。



神经风格迁移中「快」与「更快」问题的解决方案。该研究方向的关键问题是如何突破速度、灵活性、质量三者的权衡。可能的一个解决方案是跟随 Chen 和 Schmidt 的研究 [9]。他们的算法是目前最有效的算法，但图片质量并不高。改进他们的方法所产生的风格迁移图像的质量是突破速度、灵活性、质量三者权衡的有潜力的方向。目前已有一些相关工作，比如 [51]。



对神经风格迁移算法「快」与「更快」的笔触大小控制而言，思路类似于之前提到的笔触方向控制的可能解决方案。在 NPR 领域，有大量的研究人员在做笔触大小控制。若要进行回顾，我们推荐 [39] 中的章节 1。



**8 结论和未来工作**



过去三年来，神经风格迁移已经持续成长为了一个蓬勃发展的研究领域。这一研究领域内越来越多的活动受到了科学挑战和工业需求的推动。而且在神经风格迁移领域内，研究者们也已经进行了数量可观的研究。这一领域内的关键进展见于表 3。总体而言，这篇概述提供了对神经风格迁移现有研究的广泛调研，覆盖了当前方法的类别、它们的改进和扩展、评估方法以及现有的挑战和对应的可能解决方案。此外，我们还概述了神经风格迁移的三个应用领域，包括社交、辅助用户创作的工具和用于娱乐应用的产品工具。





*![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/2019-11-12-075610.png)*

*表 3：总结神经风格迁移领域内的当前进展*





未来研究神经风格迁移，有前景的方向主要集中在两个方面。一是解决前面提到的当前算法所面临的难题，即参数微调问题、笔触方向控制问题和神经风格迁移中「快」与「更快」的问题。对这些难题及其对应的可能解决方案的描述见于第 7 节。第二个有前景的方向是关注神经风格迁移的新扩展（比如，时尚风格迁移和字符风格迁移），在这个方向上已经有了一些初步的研究成果，比如最近 Yang et al. [47] 的关于文本效果迁移（Text Effects Transfer）的研究。这些有趣的扩展可能会变为未来的研究主题趋势，之后又可能会创造出新的相关领域。

选自[arXiv](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1705.04058.pdf) **机器之心编译**

编辑于 2017-05-15